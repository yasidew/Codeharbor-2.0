{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNWI5GuJuAaDVPP5h+r1gm/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasidew/Codeharbor-2.0/blob/feature%2Fcode-analysis/code_analysis_model_1.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.   Dataset Preparation**\n",
        "\n",
        "*   The dataset preparation process involves creating a structured dataset that supports machine learning or data analysis workflows. This step is critical for ensuring data consistency, enabling reproducibility, and providing diverse examples that help improve the robustness of models.\n",
        "\n",
        "**Workflow Overview:**\n",
        "\n",
        "1.    Create a dataset with varying vulnerabaility checks catgorizing as func and target\n",
        "\n",
        "*   **func**: Contains the raw code snippet representing a function or block.\n",
        "*   **target:** A Suggestion or description of the detected issue or vulnerability\n",
        "\n",
        "2.   Store the dataset in a standard text-based format such as JSON (JSON) for easy parsing:\n",
        "\n",
        "3. **Data Validation:** Normalize code snippets by ensuring consistent formatting (e.g., proper indentation, line breaks).\n",
        "\n",
        "\n",
        "4. **Augment Dataset:** Include both synthetic examples (generated or modified to demonstrate specific vulnerabilities) , real-world examples sourced from repositories and Hugging Face\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lEF7qLtjiMbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.   Pre-Process Data set**\n",
        "\n",
        "**Why Preprocess Dataset**:\n",
        "\n",
        "Preprocessing is a crucial step in preparing the dataset for model training and evaluation. It ensures that the data is:\n",
        "\n",
        "**Randomized:** Prevents biases introduced by inherent ordering in the dataset.\n",
        "\n",
        "**Well-Partitioned:** Divided into training, and test subsets to evaluate the model's performance effectively.\n",
        "\n",
        "**Tokenization:** Convert code snippets into a tokenized format suitable for input into machine learning models, such as sequence-to-sequence or transformer models.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v8Wju1QPnUNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Fine Tuning**:\n",
        "\n",
        "**What is Fine Tunning?**:\n",
        "\n",
        "fine-tuning involves adapting a pre-trained model CodeT5 model to specialize in detecting code vulnerabilities, security issues, and other specific programming checks. The model learns to identify patterns and features unique to vulnerable code\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Why CodeT5 was Chosen for Fine-Tuning**\n",
        "**Overview of CodeT5:**\n",
        "\n",
        "CodeT5, developed by Salesforce, is a powerful pre-trained sequence-to-sequence model designed specifically for code-related tasks, such as:\n",
        "\n",
        "\n",
        "\n",
        "*   **Code Generation:** Generating code snippets from input queries.\n",
        "*   **Code Translation:** Converting code from one programming language to another.\n",
        "*   **Code Summarization:** Generating concise summaries of code functionality.\n",
        "*   **Code Completion:** Auto-completing partially written code.\n",
        "\n",
        "\n",
        "\n",
        "**Key Features of CodeT5:**\n",
        "\n",
        "\n",
        "1.   **Specialized for Programming Tasks**\n",
        "      *   Pre-trained on diverse programming languages, including Java, Python, and JavaScript, making it highly effective for understanding and generating code.\n",
        "\n",
        "2.   **Sequence-to-Sequence Architecture:**\n",
        "      *   Allows flexible input-output relationships, such as transforming one code snippet into another, which aligns perfectly with our singleton transformation task.\n",
        "\n",
        "3.   **Model Size Options:**\n",
        "      *   Offers multiple variants like base, small, and large, depending on available compute resources.\n",
        "\n",
        "4.   **Transformer-Based:**\n",
        "      *   Built on the Transformer architecture, which is state-of-the-art for natural language processing (NLP) and code-related tasks.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# **Other Available Models and Why They Weren't Chosen**\n",
        "### **1. GPT Models (e.g., Codex by OpenAI)**:\n",
        "- **Advantages**:\n",
        "  - Extremely powerful and capable of complex reasoning.\n",
        "  - Performs well across various programming languages.\n",
        "- **Limitations**:\n",
        "  - Requires substantial computational resources for fine-tuning, especially on large datasets.\n",
        "  - Fine-tuning access for Codex is limited compared to its use as a pre-trained API (closed access).\n",
        "  - Higher cost associated with cloud usage and resource allocation.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. CodeBERT**:\n",
        "- **Advantages**:\n",
        "  - Designed for programming tasks like code search and clone detection.\n",
        "  - Lightweight and easier to fine-tune compared to GPT models.\n",
        "- **Limitations**:\n",
        "  - Primarily built for tasks like code retrieval and classification, not for sequence-to-sequence tasks like code transformation.\n",
        "  - Less effective for generating code due to its focus on representation learning rather than generation.\n",
        "\n",
        "\n",
        "## **Why CodeT5 is the Best Fit for This Task**\n",
        "1. **Focus on Code Generation**:\n",
        "   - CodeT5's architecture is optimized for understanding and generating code, making it ideal for tasks like identifying vulnerabilities and checks. Its ability to comprehend complex code structures and generate meaningful outputs aligns perfectly with the requirements of analyzing and improving code quality.\n",
        "2. **Extensive Pre-Training on Programming Data**:\n",
        "   - CodeT5 has been trained on datasets like CodeSearchNet, giving it a strong foundation in understanding and generating high-quality code.\n",
        "3. **Ease of Fine-Tuning**:\n",
        "   - It supports fine-tuning with Hugging Face's Trainer API, simplifying the implementation process.\n",
        "4. **Scalability**:\n",
        "   - The `base` version balances model size and performance, making it suitable for running on Colab Pro's GPU resources."
      ],
      "metadata": {
        "id": "MLB598Thpvik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "def preprocess_data_with_auto_tokenizer(data, tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenize the dataset using AutoTokenizer.\n",
        "    \"\"\"\n",
        "    def tokenize_function(example):\n",
        "        return tokenizer(example[\"func\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "    # Debug: Print the first few samples before tokenization\n",
        "    print(\"Data before tokenization:\", data[:3])\n",
        "\n",
        "    # Tokenize and format the dataset\n",
        "    tokenized_data = data.map(tokenize_function, batched=False)\n",
        "    tokenized_data = tokenized_data.rename_column(\"label\", \"labels\")\n",
        "    tokenized_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    # Debug: Print the first few samples after tokenization\n",
        "    print(\"Data after tokenization:\", tokenized_data[:3])\n",
        "\n",
        "    return tokenized_data\n",
        "\n",
        "def validate_dataset(dataset, stage=\"train\"):\n",
        "    \"\"\"\n",
        "    Validate the dataset to ensure all necessary fields are present.\n",
        "    \"\"\"\n",
        "    print(f\"Validating {stage} dataset...\")\n",
        "    for i, sample in enumerate(dataset):\n",
        "        assert \"input_ids\" in sample, f\"Sample {i} missing 'input_ids'\"\n",
        "        assert \"attention_mask\" in sample, f\"Sample {i} missing 'attention_mask'\"\n",
        "        assert \"labels\" in sample, f\"Sample {i} missing 'labels'\"\n",
        "\n",
        "        # Adjust to check for tensors or lists\n",
        "        assert isinstance(sample[\"input_ids\"], (list, torch.Tensor)), f\"Sample {i} 'input_ids' is not a list or tensor\"\n",
        "        assert isinstance(sample[\"attention_mask\"], (list, torch.Tensor)), f\"Sample {i} 'attention_mask' is not a list or tensor\"\n",
        "        assert isinstance(sample[\"labels\"], (int, torch.Tensor)), f\"Sample {i} 'labels' is not an integer or tensor\"\n",
        "\n",
        "        # Additional check for tensor shape if it's a tensor\n",
        "        if isinstance(sample[\"input_ids\"], torch.Tensor):\n",
        "            assert sample[\"input_ids\"].ndim == 1, f\"Sample {i} 'input_ids' tensor is not 1-dimensional\"\n",
        "        if isinstance(sample[\"attention_mask\"], torch.Tensor):\n",
        "            assert sample[\"attention_mask\"].ndim == 1, f\"Sample {i} 'attention_mask' tensor is not 1-dimensional\"\n",
        "\n",
        "    print(f\"{stage} dataset validation complete.\")\n",
        "\n",
        "\n",
        "def preprocess_dataset(raw_data_path, model_name, output_dir=\"dataset/processed_data\"):\n",
        "    \"\"\"\n",
        "    Preprocess the raw dataset and save it.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load raw data\n",
        "    print(f\"Loading dataset from: {raw_data_path}\")\n",
        "    with open(raw_data_path, 'r') as file:\n",
        "        data = [json.loads(line) for line in file.readlines()]\n",
        "\n",
        "    dataset = Dataset.from_dict({\"func\": [d[\"func\"] for d in data], \"label\": [d[\"label\"] for d in data]})\n",
        "\n",
        "    # Split dataset\n",
        "    split_data = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    train_dataset = split_data[\"train\"]\n",
        "    test_dataset = split_data[\"test\"]\n",
        "\n",
        "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    # Preprocess data\n",
        "    train_dataset = preprocess_data_with_auto_tokenizer(train_dataset, tokenizer)\n",
        "    test_dataset = preprocess_data_with_auto_tokenizer(test_dataset, tokenizer)\n",
        "\n",
        "    # Validate datasets\n",
        "    validate_dataset(train_dataset, stage=\"train\")\n",
        "    validate_dataset(test_dataset, stage=\"test\")\n",
        "\n",
        "    # Save preprocessed datasets\n",
        "    train_path = os.path.join(output_dir, \"custom_train.jsonl\")\n",
        "    test_path = os.path.join(output_dir, \"custom_test.jsonl\")\n",
        "    train_dataset.to_json(train_path)\n",
        "    test_dataset.to_json(test_path)\n",
        "\n",
        "    print(f\"Processed data saved at: {output_dir}\")\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# Preprocess the raw dataset\n",
        "train_dataset, test_dataset = preprocess_dataset(\n",
        "    raw_data_path=\"dataset/raw_data/custom_dataset.jsonl\",\n",
        "    model_name=\"Salesforce/codet5-base\"\n",
        ")"
      ],
      "metadata": {
        "id": "fnWJ_IpApJAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python preprocess.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y3xd00_v1kw",
        "outputId": "b313aa79-54a2-40eb-925a-1f9dc38e8e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: custom_dataset.json\n",
            "Train dataset size: 1200\n",
            "Test dataset size: 300\n",
            "tokenizer_config.json: 100% 1.48k/1.48k [00:00<00:00, 9.53MB/s]\n",
            "vocab.json: 100% 703k/703k [00:00<00:00, 1.11MB/s]\n",
            "merges.txt: 100% 294k/294k [00:00<00:00, 643kB/s]\n",
            "added_tokens.json: 100% 2.00/2.00 [00:00<00:00, 11.1kB/s]\n",
            "special_tokens_map.json: 100% 12.5k/12.5k [00:00<00:00, 48.5MB/s]\n",
            "Data before tokenization: {'func': ['def unused_variable():\\n    x = 10\\n    return 5', 'def risky_eval(data):\\n    return eval(data)', \"def sql_injection(user_input):\\n    query = 'SELECT * FROM users WHERE name = ' + user_input\\n    return query\"], 'target': ['No specific issue detected.', 'Avoid using eval as it can execute arbitrary code. Use ast.literal_eval for safer parsing.', 'Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.']}\n",
            "Map: 100% 1200/1200 [00:00<00:00, 2933.44 examples/s]\n",
            "Data after tokenization: {'input_ids': tensor([[    1,   536, 10197,  ...,     0,     0,     0],\n",
            "        [    1,   536, 18404,  ...,     0,     0,     0],\n",
            "        [    1,   536,  1847,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    1,  2279,  2923,  5672,  8316,    18,     2,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1,  3769,   839,  1450,  5302,   487,   518,   848,  1836, 11078,\n",
            "           981,    18,  2672,  3364,    18, 13107,    67,  8622,   364,  7864,\n",
            "           586,  5811,    18,     2,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1,  3769,   839,   533, 26833,   316,  3063,  6218,    18,  2672,\n",
            "         17629,  6218,   358,  5309,  3063, 10380,    18,     2,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]])}\n",
            "Data before tokenization: {'func': ['def risky_eval(data):\\n    return eval(data)', \"def sql_injection(user_input):\\n    query = 'SELECT * FROM users WHERE name = ' + user_input\\n    return query\", \"def sql_injection(user_input):\\n    query = 'SELECT * FROM users WHERE name = ' + user_input\\n    return query\"], 'target': ['Avoid using eval as it can execute arbitrary code. Use ast.literal_eval for safer parsing.', 'Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.', 'Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.']}\n",
            "Map: 100% 300/300 [00:00<00:00, 3334.29 examples/s]\n",
            "Data after tokenization: {'input_ids': tensor([[    1,   536, 18404,  ...,     0,     0,     0],\n",
            "        [    1,   536,  1847,  ...,     0,     0,     0],\n",
            "        [    1,   536,  1847,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    1,  3769,   839,  1450,  5302,   487,   518,   848,  1836, 11078,\n",
            "           981,    18,  2672,  3364,    18, 13107,    67,  8622,   364,  7864,\n",
            "           586,  5811,    18,     2,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1,  3769,   839,   533, 26833,   316,  3063,  6218,    18,  2672,\n",
            "         17629,  6218,   358,  5309,  3063, 10380,    18,     2,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1,  3769,   839,   533, 26833,   316,  3063,  6218,    18,  2672,\n",
            "         17629,  6218,   358,  5309,  3063, 10380,    18,     2,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]])}\n",
            "Creating json from Arrow format: 100% 2/2 [00:00<00:00, 17.23ba/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 43.28ba/s]\n",
            "Processed data saved at: dataset/processed_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEUPJx-5v9hg",
        "outputId": "582cb42a-dba4-4ae0-c6a9-8bab7310ad8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Model Evaluation - F1 Score**\n",
        "\n",
        "## **What is Model Evaluation?**\n",
        "The F1 Score is a robust metric for evaluating the model's ability to identify vulnerabilities and checks accurately. It considers both false positives (e.g., incorrectly flagged non-issues) and false negatives (e.g., missed vulnerabilities), which are critical for this component..\n",
        "\n",
        "---\n",
        "\n",
        "## **Why Use F1 Score?**\n",
        "The F1 Score is a harmonic mean of **precision** and **recall**, making it particularly useful when there is an uneven class distribution or when both false positives and false negatives are critical.\n",
        "\n",
        "- **Precision**: Measures the accuracy of the vulnerabilities detected by the model, ensuring fewer false positives..\n",
        "- **Recall**: Assesses the model's ability to detect all actual vulnerabilities, reducing false negatives.\n",
        "- **F1 Score**: Provides a balanced view of precision and recall, ensuring the model performs well across diverse vulnerability categories."
      ],
      "metadata": {
        "id": "RSeZeyNEvp59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. What Happens During Model Training?**\n",
        "\n",
        "**Data Preparation:**\n",
        "\n",
        "* The dataset is tokenized using a custom tokenizer for both input (func) and target (target) sequences.\n",
        "* The input_ids, attention_mask, and labels are prepared, ensuring uniform length via padding.\n",
        "\n",
        "**Batching with DataLoader:**\n",
        "\n",
        "* The tokenized data is loaded into DataLoader objects for both training and testing.\n",
        "* A custom collation function ensures correct batching with appropriate padding.\n",
        "\n",
        "**Model Setup:**\n",
        "\n",
        "* A T5ForConditionalGeneration model (CodeT5) is initialized.\n",
        "* The model is moved to the GPU for efficient training and inference.\n",
        "\n",
        "**Optimizer and Training Loop:**\n",
        "\n",
        "* AdamW optimizer is used for weight updates.\n",
        "\n",
        "- **For each epoch:**\n",
        "\n",
        "  * The model processes the input and calculates the loss.\n",
        "  * The loss is backpropagated, and model weights are updated to minimize it.\n",
        "- The average loss per epoch is printed to monitor training progress.\n",
        "\n",
        "**Model Saving:**\n",
        "\n",
        "After training, the fine-tuned model and tokenizer are saved to disk for later use.\n",
        "\n",
        "**Evaluation:**\n",
        "\n",
        "* The model is evaluated on the test dataset by generating predictions for the input code snippets.\n",
        "* Predictions are compared with ground-truth suggestions, and metrics like Precision, Recall, and F1 Score are computed.\n",
        "\n",
        "**Sample Outputs:**\n",
        "\n",
        "A few examples of input code, generated suggestions, and ground-truth suggestions are displayed for qualitative analysis."
      ],
      "metadata": {
        "id": "ngaWodK-ybIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "model_name = \"Salesforce/codet5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Custom collation function for seq2seq tasks\n",
        "def custom_collate_fn(batch):\n",
        "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in batch]\n",
        "    attention_masks = [torch.tensor(item[\"attention_mask\"]) for item in batch]\n",
        "    labels = [torch.tensor(item[\"labels\"]) for item in batch]\n",
        "\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "    labels = pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_masks,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "# Load and preprocess datasets\n",
        "def load_and_preprocess_dataset(file_path, tokenizer):\n",
        "    data = Dataset.from_json(file_path)\n",
        "\n",
        "    def tokenize_function(example):\n",
        "        inputs = tokenizer(\n",
        "            example[\"func\"], truncation=True, padding=\"max_length\", max_length=512\n",
        "        )\n",
        "        targets = tokenizer(\n",
        "            example[\"target\"], truncation=True, padding=\"max_length\", max_length=128\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"],\n",
        "            \"attention_mask\": inputs[\"attention_mask\"],\n",
        "            \"labels\": targets[\"input_ids\"],\n",
        "        }\n",
        "\n",
        "    return data.map(tokenize_function, batched=True)\n",
        "\n",
        "# Paths to the processed JSON files\n",
        "train_file_path = \"dataset/processed_data/custom_train.json\"\n",
        "test_file_path = \"dataset/processed_data/custom_test.json\"\n",
        "\n",
        "# Preprocess datasets\n",
        "train_dataset = load_and_preprocess_dataset(train_file_path, tokenizer)\n",
        "test_dataset = load_and_preprocess_dataset(test_file_path, tokenizer)\n",
        "\n",
        "# Set up DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "# Move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "epochs = 6\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} Loss: {total_loss / len(train_dataloader)}\")\n",
        "\n",
        "# Save the trained model\n",
        "model_save_path = \"models/custom_seq2seq_model\"\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating model on test dataset...\")\n",
        "model.eval()\n",
        "\n",
        "generated_targets = []\n",
        "ground_truth_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model.generate(inputs[\"input_ids\"], max_length=128)\n",
        "\n",
        "        generated_targets.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "        ground_truth_targets.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
        "\n",
        "# Calculate F1 Score\n",
        "binary_true = [1 if gt == pred else 0 for gt, pred in zip(ground_truth_targets, generated_targets)]\n",
        "binary_pred = [1] * len(binary_true)  # Assume all generated are positive predictions\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(binary_true, binary_pred, average=\"binary\")\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Display some examples\n",
        "for i in range(5):\n",
        "    print(f\"Input Code: {test_dataset[i]['func']}\")\n",
        "    print(f\"Generated Suggestion: {generated_targets[i]}\")\n",
        "    print(f\"Ground Truth Suggestion: {ground_truth_targets[i]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "vc24yZChw3T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eELh48V2wJQA",
        "outputId": "26316d3f-aae4-45b8-e452-0eeaa1b83729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-01 10:26:18.834932: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-01 10:26:18.854207: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-01 10:26:18.861105: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-01 10:26:18.875186: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-01 10:26:19.981173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "config.json: 100% 1.57k/1.57k [00:00<00:00, 9.25MB/s]\n",
            "pytorch_model.bin: 100% 892M/892M [00:08<00:00, 103MB/s]\n",
            "Generating train split: 1200 examples [00:00, 6624.31 examples/s]\n",
            "Map: 100% 1200/1200 [00:00<00:00, 1667.39 examples/s]\n",
            "Generating train split: 300 examples [00:00, 5774.92 examples/s]\n",
            "Map: 100% 300/300 [00:00<00:00, 1873.55 examples/s]\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Epoch 1/6 Loss: 0.0589347152436009\n",
            "Epoch 2/6 Loss: 7.631881876629147e-05\n",
            "Epoch 3/6 Loss: 2.9702167275900137e-05\n",
            "Epoch 4/6 Loss: 1.632187032252356e-05\n",
            "Epoch 5/6 Loss: 1.836790032408923e-05\n",
            "Epoch 6/6 Loss: 2.7465096045868147e-05\n",
            "Evaluating model on test dataset...\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n",
            "Input Code: def risky_eval(data):\n",
            "    return eval(data)\n",
            "Generated Suggestion: Avoid using eval as it can execute arbitrary code. Use ast.literal_eval for safer parsing.\n",
            "Ground Truth Suggestion: Avoid using eval as it can execute arbitrary code. Use ast.literal_eval for safer parsing.\n",
            "\n",
            "Input Code: def sql_injection(user_input):\n",
            "    query = 'SELECT * FROM users WHERE name = ' + user_input\n",
            "    return query\n",
            "Generated Suggestion: Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.\n",
            "Ground Truth Suggestion: Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.\n",
            "\n",
            "Input Code: def sql_injection(user_input):\n",
            "    query = 'SELECT * FROM users WHERE name = ' + user_input\n",
            "    return query\n",
            "Generated Suggestion: Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.\n",
            "Ground Truth Suggestion: Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.\n",
            "\n",
            "Input Code: def sql_injection(user_input):\n",
            "    query = 'SELECT * FROM users WHERE name = ' + user_input\n",
            "    return query\n",
            "Generated Suggestion: Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.\n",
            "Ground Truth Suggestion: Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.\n",
            "\n",
            "Input Code: def sql_injection(user_input):\n",
            "    query = 'SELECT * FROM users WHERE name = ' + user_input\n",
            "    return query\n",
            "Generated Suggestion: Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.\n",
            "Ground Truth Suggestion: Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python preprocess.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kikwRdRH1Sxx",
        "outputId": "ad29d937-124a-4ecd-b8a0-6fee3b2d8528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: custom_dataset.json\n",
            "Train dataset size: 1227\n",
            "Test dataset size: 307\n",
            "Data before tokenization: {'func': ['def long_function():\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n    pass\\n', \"def hard_coded_secret():\\n    secret = 'password_2123'\\n    return secret\", 'def naming_conventions():\\n    VarName = 10\\n    return VarName'], 'target': ['Function length exceeds recommended limits. Split into smaller functions.', 'Avoid hardcoding secrets in code. Use environment variables or secret management tools.', 'Follow consistent naming conventions like snake_case for better readability.']}\n",
            "Map: 100% 1227/1227 [00:00<00:00, 2686.29 examples/s]\n",
            "Data after tokenization: {'input_ids': tensor([[    1,   536,  1525,  ...,     0,     0,     0],\n",
            "        [    1,   536,  7877,  ...,     0,     0,     0],\n",
            "        [    1,   536, 14634,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    1,  2083,   769, 14399, 14553,  8181,    18,  5385,  1368, 10648,\n",
            "          4186,    18,     2,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1,  3769,   839,  7877,  2014, 14612,   316,   981,    18,  2672,\n",
            "          3330,  3152,   578,  4001, 11803,  8513,    18,     2,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1,  8328, 11071, 14634,   356, 24862,  3007, 16774,    67,  3593,\n",
            "           364,  7844,   855,  2967,    18,     2,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]])}\n",
            "Data before tokenization: {'func': ['def circular_imports():\\n    import module_a\\n    return module_a.value', 'def weak_cryptography(password):\\n    return hashlib.md5(password.encode()).hexdigest()', 'def excessive_globals():\\n    global x\\n    x = 42\\n    return x'], 'target': ['Avoid circular imports by refactoring dependencies.', 'Avoid using MD5 for hashing. Use a secure hashing algorithm like bcrypt or SHA-256.', 'Avoid excessive use of global variables. Use function parameters or class attributes.']}\n",
            "Map: 100% 307/307 [00:00<00:00, 3272.30 examples/s]\n",
            "Data after tokenization: {'input_ids': tensor([[    1,   536, 15302,  ...,     0,     0,     0],\n",
            "        [    1,   536, 16046,  ...,     0,     0,     0],\n",
            "        [    1,   536, 23183,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    1,  3769,   839, 15302, 10095,   635,   283,  3493,  6053,  5030,\n",
            "            18,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1,  3769,   839,  1450, 10852,    25,   364, 24641,    18,  2672,\n",
            "           279,  8177, 24641,  4886,  3007,  6533,  2015,   578,  9777,    17,\n",
            "          5034,    18,     2,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1,  3769,   839, 23183,   688,   999,   434,  2552,  3152,    18,\n",
            "          2672,   445,  1472,   578,   667,  1677,    18,     2,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]])}\n",
            "Creating json from Arrow format: 100% 2/2 [00:00<00:00, 19.09ba/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 38.64ba/s]\n",
            "Processed data saved at: dataset/processed_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRdLiHn-16Rq",
        "outputId": "9aa9a67b-ecc8-49ee-9abd-d80894c594ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-01 10:50:22.840696: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-01 10:50:22.860294: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-01 10:50:22.866355: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-01 10:50:22.881323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-01 10:50:23.987733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Generating train split: 1227 examples [00:00, 10235.89 examples/s]\n",
            "Map: 100% 1227/1227 [00:00<00:00, 2719.99 examples/s]\n",
            "Generating train split: 307 examples [00:00, 11323.69 examples/s]\n",
            "Map: 100% 307/307 [00:00<00:00, 2687.60 examples/s]\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Epoch 1/6 Loss: 0.11781895137373025\n",
            "Epoch 2/6 Loss: 0.0020652044554910673\n",
            "Epoch 3/6 Loss: 0.005993825088962131\n",
            "Epoch 4/6 Loss: 0.0002104240884008614\n",
            "Epoch 5/6 Loss: 0.0007172177727103836\n",
            "Epoch 6/6 Loss: 0.0001382130782840605\n",
            "Evaluating model on test dataset...\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n",
            "Input Code: def circular_imports():\n",
            "    import module_a\n",
            "    return module_a.value\n",
            "Generated Suggestion: Avoid circular imports by refactoring dependencies.\n",
            "Ground Truth Suggestion: Avoid circular imports by refactoring dependencies.\n",
            "\n",
            "Input Code: def weak_cryptography(password):\n",
            "    return hashlib.md5(password.encode()).hexdigest()\n",
            "Generated Suggestion: Avoid using MD5 for hashing. Use a secure hashing algorithm like bcrypt or SHA-256.\n",
            "Ground Truth Suggestion: Avoid using MD5 for hashing. Use a secure hashing algorithm like bcrypt or SHA-256.\n",
            "\n",
            "Input Code: def excessive_globals():\n",
            "    global x\n",
            "    x = 42\n",
            "    return x\n",
            "Generated Suggestion: Avoid excessive use of global variables. Use function parameters or class attributes.\n",
            "Ground Truth Suggestion: Avoid excessive use of global variables. Use function parameters or class attributes.\n",
            "\n",
            "Input Code: def risky_eval(data):\n",
            "    return eval(data)\n",
            "Generated Suggestion: Avoid using eval as it can execute arbitrary code. Use ast.literal_eval for safer parsing.\n",
            "Ground Truth Suggestion: Avoid using eval as it can execute arbitrary code. Use ast.literal_eval for safer parsing.\n",
            "\n",
            "Input Code: def improper_sanitization(user_input):\n",
            "    os.system('rm -rf ' + user_input)\n",
            "Generated Suggestion: Validate and sanitize inputs to avoid command injection vulnerabilities.\n",
            "Ground Truth Suggestion: Validate and sanitize inputs to avoid command injection vulnerabilities.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r custom_seq2seq_model.zip ./models/custom_seq2seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72XFCr9w7EST",
        "outputId": "256c93ec-f144-4953-84b3-a66ff62ec150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: models/custom_seq2seq_model/ (stored 0%)\n",
            "  adding: models/custom_seq2seq_model/tokenizer_config.json (deflated 94%)\n",
            "  adding: models/custom_seq2seq_model/special_tokens_map.json (deflated 97%)\n",
            "  adding: models/custom_seq2seq_model/tokenizer.json (deflated 82%)\n",
            "  adding: models/custom_seq2seq_model/merges.txt (deflated 54%)\n",
            "  adding: models/custom_seq2seq_model/vocab.json (deflated 59%)\n",
            "  adding: models/custom_seq2seq_model/model.safetensors (deflated 7%)\n",
            "  adding: models/custom_seq2seq_model/config.json (deflated 61%)\n",
            "  adding: models/custom_seq2seq_model/generation_config.json (deflated 33%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def keep_colab_active():\n",
        "    while True:\n",
        "        # Print a simple message to the log (you can also comment this out to reduce output)\n",
        "        print(\"Keeping Colab active...\")\n",
        "        time.sleep(300)  # Wait for 5 minutes (300 seconds) before the next iteration\n",
        "\n",
        "# Run the function\n",
        "keep_colab_active()"
      ],
      "metadata": {
        "id": "PywhewHsyVZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python preprocess.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx8uRf-hm6BM",
        "outputId": "c362a290-3ba8-4f69-f418-c4e81030decd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: custom_dataset.json\n",
            "Train dataset size: 1521\n",
            "Test dataset size: 381\n",
            "Data before tokenization: {'func': ['def overly_complex_parameters():\\n    def process(data, config, env, context):\\n        return True', \"def hardcoded_api_key():\\n    api_key = '12345-abcdef-67890'\\n    return api_key\", 'def insecure_data_storage():\\n    stored_password = password'], 'target': ['Simplify parameters by grouping related items into objects or dictionaries.', 'Avoid hardcoding API keys. Use secure storage or environment variables.', 'Encrypt sensitive data before storing it to protect against unauthorized access.']}\n",
            "Map: 100% 1521/1521 [00:00<00:00, 2997.50 examples/s]\n",
            "Data after tokenization: {'input_ids': tensor([[    1,   536,  1879,  ...,     0,     0,     0],\n",
            "        [    1,   536,  7877,  ...,     0,     0,     0],\n",
            "        [    1,   536, 22785,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    1, 24490,  1164,  1472,   635, 12116,  3746,  1516,  1368,  2184,\n",
            "           578, 16176,    18,     2,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1,  3769,   839,  7877,  2014,  1491,  1311,    18,  2672,  8177,\n",
            "          2502,   578,  3330,  3152,    18,     2,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1, 13129, 16692,   501,  1865, 15729,   518,   358, 17151,  5314,\n",
            "           640,  8434,  2006,    18,     2,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]])}\n",
            "Data before tokenization: {'func': ['def inconsistent_indentation():\\n    def example():\\n      return True', \"def inadequate_error_messages():\\n    try:\\n        risky_code()\\n    except ValueError:\\n        raise Exception('Error')\", 'def unnecessary_early_return():\\n    if condition:\\n        return True\\n    return False'], 'target': ['Ensure consistent indentation for better readability and maintainability.', 'Provide detailed error messages to aid in debugging.', 'Combine early returns to simplify code.']}\n",
            "Map: 100% 381/381 [00:00<00:00, 3244.04 examples/s]\n",
            "Data after tokenization: {'input_ids': tensor([[    1,   536, 27403,  ...,     0,     0,     0],\n",
            "        [    1,   536,   316,  ...,     0,     0,     0],\n",
            "        [    1,   536, 19908,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    1, 12512, 11071, 12018,   364,  7844,   855,  2967,   471, 17505,\n",
            "          2967,    18,     2,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1, 20632,  6864,   555,  2743,   358, 20702,   316, 10450,    18,\n",
            "             2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1, 21720, 11646,  1135,   358, 16499,   981,    18,     2,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]])}\n",
            "Creating json from Arrow format: 100% 2/2 [00:00<00:00, 15.17ba/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 32.02ba/s]\n",
            "Processed data saved at: dataset/processed_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CnHQQg0o5Wi",
        "outputId": "461f3bbf-6e31-46f7-972e-65661306f2ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-03 10:35:34.388374: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 10:35:34.409209: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 10:35:34.415150: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 10:35:34.429494: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 10:35:35.633166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Map: 100% 381/381 [00:00<00:00, 2586.46 examples/s]\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Epoch 1/10 Loss: 0.27427098569594655\n",
            "Epoch 2/10 Loss: 0.11517170372753945\n",
            "Epoch 3/10 Loss: 0.05935945515850867\n",
            "Epoch 4/10 Loss: 0.032880394704874576\n",
            "Epoch 5/10 Loss: 0.021194508918891037\n",
            "Epoch 6/10 Loss: 0.013630332778149292\n",
            "Epoch 7/10 Loss: 0.010106913057300129\n",
            "Epoch 8/10 Loss: 0.008016407905785694\n",
            "Epoch 9/10 Loss: 0.006848023719309519\n",
            "Epoch 10/10 Loss: 0.006778569268111599\n",
            "Evaluating model on test dataset...\n",
            "Precision: 0.8688\n",
            "Recall: 1.0000\n",
            "F1 Score: 0.9298\n",
            "Input Code: def inconsistent_indentation():\n",
            "    def example():\n",
            "      return True\n",
            "Generated Suggestion: Ensure consistent indentation for better readability and maintainability.\n",
            "Ground Truth Suggestion: Ensure consistent indentation for better readability and maintainability.\n",
            "\n",
            "Input Code: def inadequate_error_messages():\n",
            "    try:\n",
            "        risky_code()\n",
            "    except ValueError:\n",
            "        raise Exception('Error')\n",
            "Generated Suggestion: Provide detailed error messages to aid debugging.\n",
            "Ground Truth Suggestion: Provide detailed error messages to aid in debugging.\n",
            "\n",
            "Input Code: def unnecessary_early_return():\n",
            "    if condition:\n",
            "        return True\n",
            "    return False\n",
            "Generated Suggestion: Combine early returns to simplify code.\n",
            "Ground Truth Suggestion: Combine early returns to simplify code.\n",
            "\n",
            "Input Code: def unsafe_deserialization(data):\n",
            "    obj = pickle.loads(data)\n",
            "    return obj\n",
            "Generated Suggestion: Validate and sanitize data before deserialization to prevent remote code execution.\n",
            "Ground Truth Suggestion: Validate and sanitize data before deserialization to prevent remote code execution.\n",
            "\n",
            "Input Code: def long_function():\n",
            "    # This function spans over 100 lines\n",
            "    pass\n",
            "Generated Suggestion: Refactor long functions into smaller, manageable units.\n",
            "Ground Truth Suggestion: Refactor long functions into smaller, manageable units.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r models.zip ./models/custom_seq2seq_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT1nbY52Rbvz",
        "outputId": "08c942dc-9c54-4848-ae0c-017d0ebdcd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: models/custom_seq2seq_model/ (stored 0%)\n",
            "  adding: models/custom_seq2seq_model/tokenizer_config.json (deflated 94%)\n",
            "  adding: models/custom_seq2seq_model/special_tokens_map.json (deflated 97%)\n",
            "  adding: models/custom_seq2seq_model/tokenizer.json (deflated 82%)\n",
            "  adding: models/custom_seq2seq_model/merges.txt (deflated 54%)\n",
            "  adding: models/custom_seq2seq_model/vocab.json (deflated 59%)\n",
            "  adding: models/custom_seq2seq_model/model.safetensors (deflated 7%)\n",
            "  adding: models/custom_seq2seq_model/config.json (deflated 61%)\n",
            "  adding: models/custom_seq2seq_model/generation_config.json (deflated 33%)\n"
          ]
        }
      ]
    }
  ]
}