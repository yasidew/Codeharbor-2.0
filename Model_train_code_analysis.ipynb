{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "11BtobbGmJ0CVcxWuxKkbUzpvOteisTOI",
      "authorship_tag": "ABX9TyM2eNNSCqw7rzQ3/X2Y7Ycj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasidew/Codeharbor-2.0/blob/feature%2Fcode-analysis/Model_train_code_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T762I5sdwmz",
        "outputId": "87f830b6-03b7-4131-874e-50e88dc47120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python preprocess.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7gOBYvFeTZh",
        "outputId": "2e4b12f9-8cbd-4577-d7a4-f52f8c70ee29"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: custom_dataset.jsonl\n",
            "Train dataset size: 800\n",
            "Test dataset size: 200\n",
            "tokenizer_config.json: 100% 1.48k/1.48k [00:00<00:00, 9.99MB/s]\n",
            "vocab.json: 100% 703k/703k [00:00<00:00, 1.62MB/s]\n",
            "merges.txt: 100% 294k/294k [00:00<00:00, 639kB/s]\n",
            "added_tokens.json: 100% 2.00/2.00 [00:00<00:00, 10.7kB/s]\n",
            "special_tokens_map.json: 100% 12.5k/12.5k [00:00<00:00, 40.7MB/s]\n",
            "Data before tokenization: {'func': [\"public class InsecureClass {\\n    public void insecureMethod() {\\n        Runtime.getRuntime().exec('rm -rf /');\\n    }\\n}\", 'def unused_variable():\\n    x = 10\\n    return 5', 'def compute_factorial(n):\\n    return 1 if n == 0 else n * compute_factorial(n - 1)'], 'label': [1, 1, 0]}\n",
            "Map: 100% 800/800 [00:00<00:00, 1097.88 examples/s]\n",
            "Data after tokenization: {'labels': tensor([1, 1, 0]), 'input_ids': tensor([[    1,   482,   667,  ...,     0,     0,     0],\n",
            "        [    1,   536, 10197,  ...,     0,     0,     0],\n",
            "        [    1,   536,  3671,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
            "Data before tokenization: {'func': [\"def sql_injection(user_input):\\n    query = 'SELECT * FROM users WHERE name = ' + user_input\\n    return query\", 'def add_numbers(a, b):\\n    return a + b', 'def compute_factorial(n):\\n    return 1 if n == 0 else n * compute_factorial(n - 1)'], 'label': [1, 0, 0]}\n",
            "Map: 100% 200/200 [00:00<00:00, 1147.67 examples/s]\n",
            "Data after tokenization: {'labels': tensor([1, 0, 0]), 'input_ids': tensor([[   1,  536, 1847,  ...,    0,    0,    0],\n",
            "        [   1,  536,  527,  ...,    0,    0,    0],\n",
            "        [   1,  536, 3671,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
            "Validating train dataset...\n",
            "train dataset validation complete.\n",
            "Validating test dataset...\n",
            "test dataset validation complete.\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00,  4.73ba/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 29.84ba/s]\n",
            "Processed data saved at: processed_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKmVGf2YeaWI",
        "outputId": "31b3fcff-3f12-44aa-b40f-75895a67c9ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rconfig.json:   0% 0.00/1.57k [00:00<?, ?B/s]\rconfig.json: 100% 1.57k/1.57k [00:00<00:00, 9.92MB/s]\n",
            "2024-11-30 09:49:55.304450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-30 09:49:55.325260: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-30 09:49:55.331250: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-30 09:49:55.346114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-30 09:49:56.543695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "pytorch_model.bin: 100% 892M/892M [00:04<00:00, 222MB/s]\n",
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at Salesforce/codet5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100% 800/800 [00:00<00:00, 3670.25 examples/s]\n",
            "Map: 100% 200/200 [00:00<00:00, 4155.82 examples/s]\n",
            "Validating train dataset...\n",
            "train dataset validation complete.\n",
            "Validating test dataset...\n",
            "test dataset validation complete.\n",
            "Epoch 1 Loss: 0.08650581300593331\n",
            "Epoch 2 Loss: 0.000587627198001428\n",
            "Epoch 3 Loss: 0.00019781957000304827\n",
            "Model training complete and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_model.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsLv8qg0pBK4",
        "outputId": "7de9838e-2e3e-42b1-9505-54d416baad2e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-30 10:36:01.004351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-30 10:36:01.035273: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-30 10:36:01.045103: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-30 10:36:01.067507: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-30 10:36:02.703228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Code Snippet: eval(user_input)\n",
            "Classification: Vulnerable\n",
            "Suggestions: Avoid using `eval` as it can execute arbitrary code. Use `ast.literal_eval` for safer parsing.\n",
            "\n",
            "Code Snippet: os.system('rm -rf /')\n",
            "Classification: Vulnerable\n",
            "Suggestions: Avoid using `os.system` for executing shell commands. Use `subprocess` with proper validation.\n",
            "\n",
            "Code Snippet: query = 'SELECT * FROM users WHERE name = ' + user_input\n",
            "Classification: Vulnerable\n",
            "Suggestions: Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.\n",
            "\n",
            "Code Snippet: print('Hello, world!')\n",
            "Classification: Vulnerable\n",
            "Suggestions: No specific suggestions found. Review the code for potential security flaws.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn19djHbKMon",
        "outputId": "51c489d9-2c63-48bc-de57-19524e7ae29b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python preprocess.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLepVL2rKUVl",
        "outputId": "1ec45bcb-317b-4d1c-a22c-633adee8cc00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/preprocess.py:15: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  data = pd.read_json(file_path, lines=True)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/preprocess.py\", line 138, in <module>\n",
            "    train_dataset, test_dataset = preprocess_dataset(\n",
            "  File \"/content/preprocess.py\", line 99, in preprocess_dataset\n",
            "    cleaned_path = remove_exact_duplicates(raw_data_path)\n",
            "  File \"/content/preprocess.py\", line 15, in remove_exact_duplicates\n",
            "    data = pd.read_json(file_path, lines=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\", line 815, in read_json\n",
            "    return json_reader.read()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\", line 1023, in read\n",
            "    obj = self._get_object_parser(self._combine_lines(data_lines))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\", line 1051, in _get_object_parser\n",
            "    obj = FrameParser(json, **kwargs).parse()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\", line 1187, in parse\n",
            "    self._parse()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\", line 1403, in _parse\n",
            "    ujson_loads(json, precise_float=self.precise_float), dtype=None\n",
            "ValueError: Expected object or value\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python preprocess.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl9p-doTK-TG",
        "outputId": "fe78eb50-77e7-40f4-c139-7dfcd8d1d077"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: custom_dataset.jsonl\n",
            "Train dataset size: 1200\n",
            "Test dataset size: 301\n",
            "tokenizer_config.json: 100% 1.48k/1.48k [00:00<00:00, 10.3MB/s]\n",
            "vocab.json: 100% 703k/703k [00:00<00:00, 3.58MB/s]\n",
            "merges.txt: 100% 294k/294k [00:00<00:00, 4.49MB/s]\n",
            "added_tokens.json: 100% 2.00/2.00 [00:00<00:00, 14.8kB/s]\n",
            "special_tokens_map.json: 100% 12.5k/12.5k [00:00<00:00, 49.6MB/s]\n",
            "Data before tokenization: {'func': ['def unused_variable():\\n    x = 10\\n    return 5', \"def secure_file_read(file_path):\\n    with open(file_path, 'r') as f:\\n        return f.read()\", \"def sql_injection(user_input):\\n    query = 'SELECT * FROM users WHERE name = ' + user_input\\n    return query\"], 'label': [1, 0, 1]}\n",
            "Map: 100% 1200/1200 [00:00<00:00, 2258.83 examples/s]\n",
            "Data after tokenization: {'labels': tensor([1, 0, 1]), 'input_ids': tensor([[    1,   536, 10197,  ...,     0,     0,     0],\n",
            "        [    1,   536,  8177,  ...,     0,     0,     0],\n",
            "        [    1,   536,  1847,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
            "Data before tokenization: {'func': [\"def insecure_os_call():\\n    import os\\n    os.system('ls')\", \"def insecure_os_call():\\n    import os\\n    os.system('rm -rf /')\", 'def unused_variable():\\n    x = 10\\n    return 5'], 'label': [1, 1, 1]}\n",
            "Map: 100% 301/301 [00:00<00:00, 2446.90 examples/s]\n",
            "Data after tokenization: {'labels': tensor([1, 1, 1]), 'input_ids': tensor([[    1,   536, 22785,  ...,     0,     0,     0],\n",
            "        [    1,   536, 22785,  ...,     0,     0,     0],\n",
            "        [    1,   536, 10197,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
            "Validating train dataset...\n",
            "train dataset validation complete.\n",
            "Validating test dataset...\n",
            "test dataset validation complete.\n",
            "Creating json from Arrow format: 100% 2/2 [00:00<00:00, 22.00ba/s]\n",
            "Creating json from Arrow format: 100% 1/1 [00:00<00:00, 49.51ba/s]\n",
            "Processed data saved at: processed_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3VSDFtnMQ9W",
        "outputId": "49606a91-6689-46ce-c27d-1b3c59abdc2d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rconfig.json:   0% 0.00/1.57k [00:00<?, ?B/s]\rconfig.json: 100% 1.57k/1.57k [00:00<00:00, 8.89MB/s]\n",
            "2024-11-30 13:10:17.741913: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-30 13:10:17.760289: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-30 13:10:17.766017: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-30 13:10:17.780098: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-30 13:10:19.029651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "pytorch_model.bin: 100% 892M/892M [00:04<00:00, 222MB/s]\n",
            "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at Salesforce/codet5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map: 100% 1200/1200 [00:00<00:00, 2096.85 examples/s]\n",
            "Map: 100% 301/301 [00:00<00:00, 2338.45 examples/s]\n",
            "Validating train dataset...\n",
            "train dataset validation complete.\n",
            "Validating test dataset...\n",
            "test dataset validation complete.\n",
            "Epoch 1 Loss: 0.05518416221011042\n",
            "Epoch 2 Loss: 0.0002687223123454411\n",
            "Epoch 3 Loss: 8.769431926036002e-05\n",
            "Evaluating model on test dataset...\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Test Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1 Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he5ABCU0RIsr",
        "outputId": "2458bcbe-16ca-48cb-bba6-0c918c5b5e11"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-30 13:31:23.080813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-30 13:31:23.100293: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-30 13:31:23.108105: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-30 13:31:23.122584: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-30 13:31:24.245468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Code Snippet: eval(user_input)\n",
            "Classification: Vulnerable\n",
            "Suggestions: Avoid using `eval` as it can execute arbitrary code. Use `ast.literal_eval` for safer parsing.\n",
            "\n",
            "Code Snippet: os.system('rm -rf /')\n",
            "Classification: Vulnerable\n",
            "Suggestions: Avoid using `os.system` for executing shell commands. Use `subprocess` with proper validation.\n",
            "\n",
            "Code Snippet: query = 'SELECT * FROM users WHERE name = ' + user_input\n",
            "Classification: Vulnerable\n",
            "Suggestions: Avoid string concatenation in SQL queries. Use parameterized queries to prevent SQL injection.\n",
            "\n",
            "Code Snippet: print('Hello, world!')\n",
            "Classification: Vulnerable\n",
            "Suggestions: No specific suggestions found. Review the code for potential security flaws.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r custom_defect_detection_model.zip ./models/custom_defect_detection_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC0-x1BETbfk",
        "outputId": "2950849d-1929-4504-e91d-93642a36c58c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: models/custom_defect_detection_model/ (stored 0%)\n",
            "  adding: models/custom_defect_detection_model/tokenizer_config.json (deflated 94%)\n",
            "  adding: models/custom_defect_detection_model/special_tokens_map.json (deflated 97%)\n",
            "  adding: models/custom_defect_detection_model/tokenizer.json (deflated 82%)\n",
            "  adding: models/custom_defect_detection_model/merges.txt (deflated 54%)\n",
            "  adding: models/custom_defect_detection_model/vocab.json (deflated 59%)\n",
            "  adding: models/custom_defect_detection_model/model.safetensors (deflated 7%)\n",
            "  adding: models/custom_defect_detection_model/config.json (deflated 62%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "znK1p7o5ZON6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}